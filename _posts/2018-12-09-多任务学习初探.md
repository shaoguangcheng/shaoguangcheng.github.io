---
layout:     post
title:     	多任务学习初探
subtitle:   
date:       2018-12-09
author:     Shaoguang Cheng
header-img: img/metapath2vec-post-bg-graph-embedding.png
catalog: true
tags:
    - multi-task learning
---

在机器学习领域，多数时候我们会关注一个特定的优化目标，比如ctr、cvr、gmv等。为了达到这个目标，我们会训练单一模型或者集成模型，并通过精细调参，使得模型逼近我们的真实目标。对于一个特定任务来说，这种做法通常可以得到可接受的结果，但我们忽略了一些可以使模型更优的信息，这些信息来自于与目标相关任务的监督数据。举例说明，在cvr任务中，我们要预估用户对一件商品发生购买的概率，一种做法是在曝光数据上单独对cvr进行建模，但这种做法丢掉商品被点击的信息（商品只有在点击之后才能发生购买），所以可以用ctr作为辅助任务进行cvr模型的训练。（更为通常的做法是在用户点击数据上训练cvr模型，但这会面临模型训练数据和应用数据分布不一致的问题，不再本文讨论范围内）**通过在相关任务之间共享数据表达信息，从而在目标任务上得到泛化性能更好的模型，这种方法被称为多任务学习（Multi-Task Learning，以下简称 MTL）**


多任务学习有多种不同的名称：joint learing、learning to learn、 learning with auxiliary tasks，但只要你在优化多个目标函数，都可以通过多任务学习的方法进行求解。即使优化的目标只有一个，也可以根据经验构造若干个辅助任务来提升主任务的性能。


从机器学习的角度来看，多任务学习被视为一种归约迁移（inductive transfer）。归约迁移（inductive transfer）通过引入归约偏置（inductive bias）来改进模型，使得模型更倾向于某些假设。举例来说，常见的一种归约偏置（Inductive bias）是L1正则化，它使得模型更偏向于那些稀疏的解。在多任务学习场景中，归约偏置（Inductive bias）是由辅助任务来提供的，这会导致模型更倾向于那些可以同时解释多个任务的解。


## 深度学习中常见的两种 MTL 模式
* Hard parameter sharing
* Soft Parameter sharing

## 为什么 MTL 有用
* 隐式数据增强
* attention focusing
* eavesdropping
* 表达偏置
* 正则化

## 非深度学习方法中的 MTL
此处的非深度学习方法主要指线性模型、kernel 方法和贝叶斯方法，这类方法在 MTL 的研究中主要围绕在两个问题的解决上：（1）通过范数正则化在多任务之间确保稀疏性 （2）建模多任务之间的关系

* Block-Sparsity regularization
* 学习多任务之间的关系


## 基于深度学习 MTL 的一些进展

## Cross Stitch network

### Weighting losses with uncertainty

### ESMM

### Multi-Mixture of Experts for MTL 

### Feature Calibration in MTL 

### Viewing MTL as multi-objective optimization

## 如何构建辅助任务
构建辅助任务的原则是：辅助任务应该是与主任务密切相关的，或者是能够对主任务的学习过程有益的。
* 相关任务
* 对抗任务
* 提示性（hints）任务
* 表示学习
* 预测输入